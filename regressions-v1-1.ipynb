{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-25T10:39:51.682974Z","iopub.execute_input":"2022-08-25T10:39:51.683462Z","iopub.status.idle":"2022-08-25T10:39:51.694414Z","shell.execute_reply.started":"2022-08-25T10:39:51.683422Z","shell.execute_reply":"2022-08-25T10:39:51.693077Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"Started\")\nimport numpy as np, pandas as pd\nprint(\"Pandas version - \", pd.__version__)\nfrom pandas import MultiIndex, Int16Dtype\n\ntry:\n    import seaborn as sns\nexcept:\n    !pip install seaborn\n    import seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\n#import cupy, cudf\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\n\n# Importing RFE and LinearRegression\ntry:\n    import statsmodels.api as sm\nexcept:\n    !python -m pip install statsmodels\n    import statsmodels.api as sm\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model,tree,svm\n\ntry:\n    import pickle\nexcept:\n    !pip install pickle5\n    import pickle\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"All necessary libraries imported\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:39:51.697017Z","iopub.execute_input":"2022-08-25T10:39:51.697772Z","iopub.status.idle":"2022-08-25T10:39:51.747825Z","shell.execute_reply.started":"2022-08-25T10:39:51.697724Z","shell.execute_reply":"2022-08-25T10:39:51.746485Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(\"Started\")\nimport numpy as np, pandas as pd\nprint(\"Pandas version - \", pd.__version__)\nfrom pandas import MultiIndex, Int16Dtype\n\ntry:\n    import seaborn as sns\nexcept:\n    !pip install seaborn\n    import seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\n#import cupy, cudf\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\n\n# Importing RFE and LinearRegression\ntry:\n    import statsmodels.api as sm\nexcept:\n    !python -m pip install statsmodels\n    import statsmodels.api as sm\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model,tree,svm\n\ntry:\n    import pickle\nexcept:\n    !pip install pickle5\n    import pickle\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"All necessary libraries imported\")\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:39:51.750330Z","iopub.execute_input":"2022-08-25T10:39:51.751106Z","iopub.status.idle":"2022-08-25T10:39:51.774270Z","shell.execute_reply.started":"2022-08-25T10:39:51.751059Z","shell.execute_reply":"2022-08-25T10:39:51.772668Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Not going to use testdf and sample df for thesis\ntraindf = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntestdf = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsample_sub = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\nsplit= False\n\nprint(traindf.shape, \" - Training dataset shape\")\nprint(testdf.shape, \" - Testing dataset shape\")\nprint(sample_sub.shape, \" - Sample Submission dataset shape\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:39:51.776292Z","iopub.execute_input":"2022-08-25T10:39:51.776942Z","iopub.status.idle":"2022-08-25T10:40:06.674958Z","shell.execute_reply.started":"2022-08-25T10:39:51.776887Z","shell.execute_reply":"2022-08-25T10:40:06.673478Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Adding features in the dataframe\n\ndef add_features(df):\n    #https://www.kaggle.com/code/papernist/google-brain\n    df = df.copy()\n    \n    df_group = df.groupby(['breath_id'])\n    \n    \n    feature_list = ['u_in', 'time_step', 'cross']\n    \n    df['cross']= df['u_in'] * df['u_out']\n    df['area_out']= df['time_step'] * df['u_out']\n    df['area'] = df['time_step'] * df['u_in']\n    \n    # add\n    df['air_flow_rate'] = df['u_out'] - (df['u_in']/100.0)\n    df['air_flow_area'] = df['air_flow_rate'] * df['time_step']\n    print(\"Step-1...Completed\")\n    \n    # cumsum cummean\n    df['one'] = 1\n    df['count'] = df_group['one'].cumsum()\n    for feature in feature_list:\n        df[f'{feature}_cumsum'] = df_group[feature].cumsum()\n        df[f'{feature}_cummean'] = df[f'{feature}_cumsum'] / df['count']\n        \n    print(\"Step-2 cumsum cummean ...Completed\")\n    # lagging\n    use_lags = 4\n    for lag in range(1, use_lags+1):\n        for feature in feature_list:\n            # lag \n            df[f'{feature}_lag_{lag}'] = df_group[feature].shift(lag)\n            # inverse lag\n            df[f'{feature}_lag_inverse_{lag}'] = df_group[feature].shift(-lag)\n\n            # dif lag\n            df[f'{feature}_lag_diff_{lag}'] = df[feature] - df[f'{feature}_lag_{lag}']\n\n            # dif inverse lag\n            df[f'{feature}_lag_inverse_diff_{lag}'] = df[feature] - df[f'{feature}_lag_inverse_{lag}']\n\n            df = df.drop(columns=[f'{feature}_lag_{lag}', f'{feature}_lag_inverse_{lag}'])\n        \n    df = df.fillna(0)\n    print(\"Step-3 lagging ...Completed\")\n    \n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    df = df.fillna(0)\n    print(\"Step-4 categorical ...Completed\")\n    \n    rolling_list = [15]\n    for roll in rolling_list:\n        for feature in feature_list:\n            df[[f'{roll}_{feature}_sum',f'{roll}_{feature}_min',\n                f'{roll}_{feature}_max',f'{roll}_{feature}_mean']] = (df_group[feature]\\\n                                                                  .rolling(window=roll,min_periods=1)\\\n                                                                  .agg({f'{roll}_{feature}_sum':'sum',\n                                                                        f'{roll}_{feature}_min':'min',\n                                                                        f'{roll}_{feature}_max':'max',\n                                                                        f'{roll}_{feature}_mean':'mean'})\\\n                                                                   .reset_index(level=0,drop=True))\n    \n    print(\"Step-5 Sliding window...Completed\")\n    print()\n    \n    df = df.fillna(0)\n    df = df.drop(['id', 'breath_id','one','count'], axis=1)\n    \n    return df.astype(np.float16)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:40:06.678055Z","iopub.execute_input":"2022-08-25T10:40:06.678593Z","iopub.status.idle":"2022-08-25T10:40:06.919128Z","shell.execute_reply.started":"2022-08-25T10:40:06.678558Z","shell.execute_reply":"2022-08-25T10:40:06.917565Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def cross_val(model):\n    pred = cross_val_score(model, X_train, y_train, cv=10)\n    return pred.mean()\n\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square    ","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:40:06.921233Z","iopub.execute_input":"2022-08-25T10:40:06.922017Z","iopub.status.idle":"2022-08-25T10:40:06.939320Z","shell.execute_reply.started":"2022-08-25T10:40:06.921961Z","shell.execute_reply":"2022-08-25T10:40:06.937961Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain = add_features(traindf)\n#test = add_features(testdf)\n\nprint(\"*\"*70)\nprint(train.shape, \" - Feature added training dataset shape\")\n#print(test.shape, \" - Features added testing dataset shape\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:40:06.941363Z","iopub.execute_input":"2022-08-25T10:40:06.941870Z","iopub.status.idle":"2022-08-25T10:42:14.441142Z","shell.execute_reply.started":"2022-08-25T10:40:06.941816Z","shell.execute_reply":"2022-08-25T10:42:14.440053Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"binary_col = []\nnum_col = []\n\nfor columnname in train.columns:\n  if len(set(train[columnname])) != 2:\n    num_col.append(columnname)\n  else:\n    binary_col.append(columnname)\n\ntrain_numcol = num_col.copy()\ntrain_numcol.remove('pressure')\n\nprint(len(binary_col), \"Number of Binary Categorical Columns\")\nprint(len(num_col),\"Number of Numerical Columns\")\nprint(len(binary_col)+len(num_col), \"Sum of Binary & Numerical Columns\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:42:14.443261Z","iopub.execute_input":"2022-08-25T10:42:14.443755Z","iopub.status.idle":"2022-08-25T10:43:31.582970Z","shell.execute_reply.started":"2022-08-25T10:42:14.443712Z","shell.execute_reply":"2022-08-25T10:43:31.581607Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = train_test_split(train, train_size = 0.7, test_size = 0.3, random_state = 100)\nprint(\"Split Done\")\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split\n\ny_train = df_train['pressure']\nX_train = df_train.drop('pressure',axis=1)\nX_train_copy = X_train.copy()\n\ny_test = df_test['pressure']\nX_test = df_test.drop('pressure',axis=1)\nX_test_copy = X_test.copy()\n\nprint(\"*\"*70)\nprint(\"Splitting the data into X train & Y train\")\nprint(X_train.shape, \"X train shape\")\nprint(y_train.shape, \"Y train shape\")\n\nprint(\"*\"*70)\nprint(\"Splitting the data into X test & Y test\")\nprint(X_test.shape, \"X test shape\")\nprint(y_test.shape, \"Y test shape\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:43:31.584753Z","iopub.execute_input":"2022-08-25T10:43:31.586039Z","iopub.status.idle":"2022-08-25T10:43:44.742107Z","shell.execute_reply.started":"2022-08-25T10:43:31.585976Z","shell.execute_reply":"2022-08-25T10:43:44.740502Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('std_scalar', MinMaxScaler())\n])\n\nX_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:43:44.743790Z","iopub.execute_input":"2022-08-25T10:43:44.744174Z","iopub.status.idle":"2022-08-25T10:44:11.103932Z","shell.execute_reply.started":"2022-08-25T10:43:44.744141Z","shell.execute_reply":"2022-08-25T10:44:11.102541Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"results_df = []","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:44:11.107842Z","iopub.execute_input":"2022-08-25T10:44:11.108311Z","iopub.status.idle":"2022-08-25T10:44:11.114786Z","shell.execute_reply.started":"2022-08-25T10:44:11.108268Z","shell.execute_reply":"2022-08-25T10:44:11.113470Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1, \n              precompute=True, \n#               warm_start=True, \n              positive=True, \n              selection='random',\n              random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n#results_df = results_df.append(results_df_2, ignore_index=True)\nprint(results_df_2)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:44:11.115875Z","iopub.execute_input":"2022-08-25T10:44:11.116779Z","iopub.status.idle":"2022-08-25T10:44:11.135972Z","shell.execute_reply.started":"2022-08-25T10:44:11.116742Z","shell.execute_reply":"2022-08-25T10:44:11.134881Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n#results_df = results_df.append(results_df_2, ignore_index=True)\nprint(results_df_2)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:44:11.137967Z","iopub.execute_input":"2022-08-25T10:44:11.138658Z","iopub.status.idle":"2022-08-25T10:44:11.159037Z","shell.execute_reply.started":"2022-08-25T10:44:11.138623Z","shell.execute_reply":"2022-08-25T10:44:11.157710Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"results_df = []\n#results_df.append(results_df_2)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:44:11.160665Z","iopub.execute_input":"2022-08-25T10:44:11.161047Z","iopub.status.idle":"2022-08-25T10:44:11.172596Z","shell.execute_reply.started":"2022-08-25T10:44:11.161013Z","shell.execute_reply":"2022-08-25T10:44:11.171304Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"'''\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=2)\n\nX_train_2_d = poly_reg.fit_transform(X_train)\nX_test_2_d = poly_reg.transform(X_test)\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train_2_d,y_train)\n\ntest_pred = lin_reg.predict(X_test_2_d)\ntrain_pred = lin_reg.predict(X_train_2_d)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Polynomail Regression\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nprint(results_df_2)\nresults_df.append(results_df_2)\n\n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:44:11.174154Z","iopub.execute_input":"2022-08-25T10:44:11.174544Z","iopub.status.idle":"2022-08-25T10:44:11.192282Z","shell.execute_reply.started":"2022-08-25T10:44:11.174511Z","shell.execute_reply":"2022-08-25T10:44:11.190736Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\nsgd_reg.fit(X_train, y_train)\n\ntest_pred = sgd_reg.predict(X_test)\ntrain_pred = sgd_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df.append(results_df_2, ignore_index=True)\nprint(results_df_2)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T10:44:11.193607Z","iopub.execute_input":"2022-08-25T10:44:11.195563Z","iopub.status.idle":"2022-08-25T11:07:27.550662Z","shell.execute_reply.started":"2022-08-25T10:44:11.195521Z","shell.execute_reply":"2022-08-25T11:07:27.548878Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"results_df_2","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:07:48.106878Z","iopub.execute_input":"2022-08-25T11:07:48.107338Z","iopub.status.idle":"2022-08-25T11:07:48.131072Z","shell.execute_reply.started":"2022-08-25T11:07:48.107272Z","shell.execute_reply":"2022-08-25T11:07:48.129483Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(0.00001), loss='mse')\n\nr = model.fit(X_train, y_train,\n              validation_data=(X_test,y_test),\n              batch_size=1,\n              epochs=100)\n\npred_ann = model.predict(X_test)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Artficial Neural Network\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:07:54.481430Z","iopub.execute_input":"2022-08-25T11:07:54.481841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'True Values': y_test, 'Predicted Values': pred_ann}).hvplot.scatter(x='True Values', y='Predicted Values')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=1000)\nrf_reg.fit(X_train, y_train)\n\ntest_pred = rf_reg.predict(X_test)\ntrain_pred = rf_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df.append(results_df_2, ignore_index=True)\nprint(results_df_2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)\nsvm_reg.fit(X_train, y_train)\n\ntest_pred = svm_reg.predict(X_test)\ntrain_pred = svm_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"SVM Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df.set_index('Model', inplace=True)\nresults_df['R2 Square'].plot(kind='barh', figsize=(12, 8))","metadata":{},"execution_count":null,"outputs":[]}]}